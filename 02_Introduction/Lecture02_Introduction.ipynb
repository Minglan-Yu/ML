{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 02  - Course Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How many of you successfully cloned the course notes? https://www.wooclap.com/L01Q00\n",
    "\n",
    "- *Course Overview*:  Introduction to machine learning and its role in variety of real-world problems in areas such as remote sensing and image processing.\n",
    "\n",
    "- So,  what is machine learning?  https://www.wooclap.com/L01Q01\n",
    "\n",
    "<!-- One definition of Machine Learning: Area of study to develop methods for computers to make (intelligent?) decisions without being explicitly programmed. -->\n",
    "\n",
    "- Supervised Learning: Learning a mapping from input data to desired output values given labeled training data\n",
    "\n",
    "\n",
    "<img src=\"figures/supervised_learning_parrot.png\" alt=\"Supervised Learning example\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "- The above was a classification example.  Each data point was classified into a discrete class (either conure or macaw).  Regression is also commonly a supervised learning problem. \n",
    "\n",
    "\n",
    "<img src=\"figures/silhoutte_regression.png\" alt=\"Supervised Learning example\" style=\"width: 300px;\"/>\n",
    "\n",
    "\n",
    "- The usual flow (but not always) for supervised learning is:\n",
    "\n",
    "    - Training\n",
    "<img src=\"figures/trainingoverview.png\" alt=\"Supervised Learning example\" style=\"width: 8000px;\"/>\n",
    "\n",
    "    - Testing\n",
    "<img src=\"figures/testingoverview.png\" alt=\"Supervised Learning example\" style=\"width: 8000px;\"/>\n",
    "\n",
    "- Subset of challenges include:\n",
    "    - How do you know if you have *representative* training data? \n",
    "    - How do you know if you extracted *good* features?\n",
    "    - How do you know if you selected the *right* model? \n",
    "    - How do you know if you trained the model *well*?\n",
    "    \n",
    "- Many of these issues are alleviated (not solved entirely, but helped significantly) with *LOTS AND LOTS* of data and good experimental design.\n",
    "\n",
    "<img src=\"figures/datacomic.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "    - Obtaining labeled training data is hard, expensive, time consuming and, in some cases, infeasible\n",
    "\n",
    "<img src=\"figures/hsicube.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "<img src=\"figures/neon.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "*from NEON: https://www.neonscience.org*\n",
    "\n",
    "\n",
    "- Unsupervised Learning: Learning structure from data without any labels \n",
    "\n",
    "<img src=\"figures/balls.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "<!-- <img src=\"figures/dataUSL.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "<img src=\"figures/dataUSL2.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "<img src=\"figures/dataUSL3.png\" style=\"width: 400px;\"/> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are many sub-areas in machine learning:\n",
    "    - **Supervised Learning**: learn from labeled data\n",
    "    - **Unsupervised Learning**: learn from unlabeled data\n",
    "    - **Semi-supervised Learning**: some training data labeled, some not, use all during training\n",
    "    - **Reinforcement Learning**: reinforcement based on action in an environment so to maximize/minimize a reward/penalty\n",
    "    - **Multiple Instance Learning**: labels have a particular from of imprecision\n",
    "    - **Active Learning**: obtaining labels online from a user/oracle in an intelligent fashion\n",
    "    - **Transfer Learning**: having labels on a related problem and transferring it to the task of interest\n",
    "    - **Structured Learning**\n",
    "    - **Associative Learning**\n",
    "    - *many more*...\n",
    "\n",
    "\n",
    "- Most of us interact with systems that leverage machine learning regularly:\n",
    "    - Alexa/Siri\n",
    "    - Search result sorting\n",
    "    - Recommendation systems (Netflix, Amazon, Twitter news feed, Facebook news feed)\n",
    "    - Google map updates via street view /  predict crowds on buses/trains/streets (https://ai.googleblog.com/2017/05/updating-google-maps-with-deep-learning.html; https://techcrunch.com/2019/06/27/google-maps-can-now-predict-how-crowded-your-bus-or-train-will-be/)\n",
    "    - Facial recognition (iPhone FaceID, JetBlue http://mediaroom.jetblue.com/investor-relations/press-releases/2018/11-15-2018-184045420) \n",
    "    - Hiring systems (https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G)\n",
    "    - Learning-driven home automation systems (e.g., NEST)\n",
    "    - Anywhere there is a computer making an automated decision, it could have been trained using machine learning\n",
    "    \n",
    "- What do you predict will be the capabilities and limitations of machine learning-driven technology in 20 years?\n",
    "    - For comparison/reference, in 1999 (20 years ago), the state of technology was: \n",
    "        - people are using mostly flip phones (iPod introduced 2001; iPhone introduced 2007)\n",
    "        - no camera phones (first camera phone sold in 2000)\n",
    "        - Netflix just launched as a DVD rental service\n",
    "        - AOL instant messenger was popular\n",
    "        - Many had dial up internet at home\n",
    "        - Napster was a popular peer-to-peer sharing service\n",
    "        - CD-ROMs commonly used\n",
    "        - USB thumb drives were not available (came on market late 2000)\n",
    "        - Very few people were using Google (founded in 1998; people were more commonly using Yahoo, Altavista, etc)\n",
    "        <!-- - Camera phones did not exist (introduced in 2000), same with the second example, maybe -->\n",
    "    - https://www.wooclap.com/L1Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Polynomial Curve Fitting Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lets begin by considering the polynomial curve fitting example in the first chapter of the text. \n",
    "* Suppose we have a training set with $N$ data samples, $\\mathbf{x} = (x_1, x_2, \\ldots, x_N)^T$ and corresponding desired outputs $\\mathbf{t} = (t_1, t_2, \\ldots, t_N)^T$ where sample $x_i$ has the desired label $t_i$\n",
    "\n",
    "* We generally organize data into *vectors* and *matrices*. Not only is it a common way to organize the data, but it allows us to easily apply linear algebraic operations during analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Suppose the data actually came from some unknown hidden function. \n",
    "* In practice/application, we only have the training data and its corresponding desired values (both of which may be noisy).  From this training data, we want to learn a mapping from input values $x$ to the desired output values $t$. \n",
    "* If we knew the hidden function, we would not need to learn the mapping - we would already know it.  However, since we do not know the true underlying function, we need to do our best to estimate from the examples of input-output pairs that we have.\n",
    "* We will learn (i.e., train a model to estimate) that mapping from the training data $\\left\\{ \\mathbf{x}, \\mathbf{t} \\right\\}$. \n",
    "* Then, when we are given test data, we can predict each test data point's $t$ value using the mapping that we estimated. \n",
    "\n",
    "* For this problem, we assume that the original data $x$ is sufficient and appropriate (so, we do not need to preprocess or extract features).  Then, we have completed steps 1 and 2 of the general approach listed in Section 0 above.\n",
    "\n",
    "* Now we must assume a model.  Lets assume a polynomial function as our model (following the example in the text):  \n",
    "\n",
    "$y(x,\\mathbf{w}) = w_0 + w_1x + w_2x^2 + \\ldots + w_Mx^M = \\sum_{j=0}^M w_jx^j$\n",
    "\n",
    "* Now we must *train* this model by estimating the unknown parameters ($\\mathbf{w}$) that maps the training data, $\\mathbf{x}$, to their desired values, $\\mathbf{t}$, given some assumed value for $M$\n",
    "\n",
    "* So, we have $N$ discrete points from which to estimate $\\mathbf{w}$.  We can minimize the squared error to estimate the parameters:\n",
    "\t\\begin{eqnarray}\n",
    "\t\t\\arg \\min_\\mathbf{w} E(\\mathbf{w}) &=& \\frac{1}{2} \\sum_{n=1}^N \\left( y(x_n, \\mathbf{w}) - t_n\\right)^2\\\\\n",
    "\t\t&=& \\frac{1}{2} \\sum_{n=1}^N \\left(\\sum_{j=0}^M w_jx_n^j -t_n \\right)^2\n",
    "\t \\end{eqnarray}\n",
    "* Consider the following illustration of the error function: \n",
    "<img src=\"figures/fig1.jpg\"  style=\"width: 200px;\"/>\n",
    "The red lines correspond to the error between the data and the functional approximation.  \n",
    "\n",
    "*  We can write the error function compactly in matrix/vector form: \n",
    "\t \\begin{eqnarray} \\nonumber\n",
    "\t \tE(\\mathbf{w}) &=& \\frac{1}{2} \\left( \\left[w_0, w_1, \\ldots, w_M \\right] \\left[ \\begin{array}{c c c c} 1 & 1 & \\ldots & 1\\\\ x_1 & x_2 & \\ldots & x_N \\\\  x_1^2 & x_2^2 & \\ldots & x_N^2 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\  x_1^M & x_2^M & \\ldots & x_N^M \\end{array}\\right] - \\left[ t_1, t_2, \\ldots, t_N\\right]\\right)\\\\\n",
    "\t \t& & \\left( \\left[w_0, w_1, \\ldots, w_M \\right] \\left[ \\begin{array}{c c c c} 1 & 1 & \\ldots & 1\\\\ x_1 & x_2 & \\ldots & x_N \\\\  x_1^2 & x_2^2 & \\ldots & x_N^2 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\  x_1^M & x_2^M & \\ldots & x_N^M \\end{array}\\right]- \\left[ t_1, t_2, \\ldots, t_N\\right]\\right)^T \\nonumber\\\\\n",
    "        &=& \\frac{1}{2}  \\left( \\mathbf{w}^T\\mathbf{X}^T - \\mathbf{t}^T\\right)\\left( \\mathbf{w}^T\\mathbf{X}^T - \\mathbf{t}^T\\right)^T\\\\\n",
    "        &=& \\frac{1}{2}\\left\\| \\mathbf{w}^T\\mathbf{X}^T - \\mathbf{t}^T \\right\\|_2^2\n",
    "\t \\end{eqnarray}\n",
    "     where  \n",
    "     \\begin{eqnarray}\\mathbf{X}^T &=& \\left[ \\begin{array}{c c c c} 1 & 1 & \\ldots & 1\\\\ x_1 & x_2 & \\ldots & x_N \\\\  x_1^2 & x_2^2 & \\ldots & x_N^2 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\  x_1^M & x_2^M & \\ldots & x_N^M \\end{array}\\right]\\\\\n",
    "     &=& \\left[ \\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_N\\right]\n",
    "     \\end{eqnarray}\n",
    "    and\n",
    "    \\begin{eqnarray}\\mathbf{x}_i = \\left[x_i^0, x_i^1, x_i^2, \\ldots, x_i^M \\right]^T \\end{eqnarray}\n",
    "\n",
    "* So, we want $E(\\mathbf{w})$ to be small.  How do we solve for $\\mathbf{w}$?\n",
    "* We can take the derivative of the error function, set it to zero, and solve for the parameters.  In general, this method does not guarantee that the parameters we estimate are minima of the error function (e.g., may be an inflection point, maxima).  It is a necessary condition (but not sufficient). However, if the function is convex, then it will always find the global optima. \n",
    "\n",
    "* How do we take the derivative of a function with respect to a vector? \n",
    "\t \\begin{equation*}\n",
    "\\frac{\\partial}{\\partial \\mathbf{x}}f(\\mathbf{x}) =\\! \\left[\\frac{\\partial}{\\partial x_1}f(\\mathbf{x}),\\frac{\\partial}{\\partial x_2}f(\\mathbf{x}),\\ldots,\\frac{\\partial}{\\partial x_n}f(\\mathbf{x})\\right]^\\top\\!\\!\\in\\! \\mathcal{R}^{n \\times 1}.\n",
    "\\end{equation*}\n",
    "\n",
    "* So, what would the derivative of $E(\\mathbf{w})$ be with respect to $\\mathbf{w}$? \n",
    "\\begin{eqnarray}\n",
    " E(\\mathbf{w}) &=& \\frac{1}{2} \\sum_{n=1}^N \\left(\\sum_{j=0}^M w_jx_n^j -t_n \\right)^2\\\\\n",
    "\\frac{\\partial E(\\mathbf{w})}{\\partial \\mathbf{w}} &=& \\left[ \\frac{\\partial E(\\mathbf{w})}{\\partial w_0},  \\frac{\\partial E(\\mathbf{w})}{\\partial w_1}, \\ldots,  \\frac{\\partial E(\\mathbf{w})}{\\partial w_M} \\right]^T\\\\\n",
    "&=& \\left[ \\sum_{n=1}^N \\left( \\sum_{j=0}^M w_jx_n^j -t_n \\right)x_n^0 ,  \\sum_{n=1}^N \\left(  \\sum_{j=0}^M w_jx_n^j -t_n \\right)x_n^1 , \\ldots, \\sum_{n=1}^N \\left(  \\sum_{j=0}^M w_jx_n^j -t_n \\right)x_n^M  \\right]^T  \\nonumber\n",
    "\\end{eqnarray}\n",
    "\n",
    "* Similarly, \n",
    "\\begin{eqnarray}\n",
    "\t\t  \\frac{\\partial E(\\mathbf{w})}{\\partial \\mathbf{w}} &=& \\left[\\frac{1}{2} 2 \\left( \\mathbf{w}^T\\mathbf{X}^T - \\mathbf{t}^T\\right)\\mathbf{X}\\right]^T\n",
    "\t \\end{eqnarray}\n",
    "\twhere $\\mathbf{X}^T= \\left[ \\begin{array}{c c c c} 1 & 1 & \\ldots & 1\\\\ x_1 & x_2 & \\ldots & x_N \\\\  x_1^2 & x_2^2 & \\ldots & x_N^2 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\  x_1^M & x_2^M & \\ldots & x_N^M \\end{array}\\right]$.\n",
    "* Then, we can set the derivative to zero and solve: \n",
    "\\begin{eqnarray}\n",
    "\t\t & & 0 = \\mathbf{X}^T\\mathbf{X}\\mathbf{w} - \\mathbf{X}^T\\mathbf{t}\\\\\n",
    "\t\t & & \\mathbf{X}^T\\mathbf{t} = \\mathbf{X}^T\\mathbf{X}\\mathbf{w} \\\\\n",
    "\t\t & & \\mathbf{w} = \\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{t}\n",
    "\t \\end{eqnarray}\n",
    "\n",
    "\n",
    "* Let's look at this more closely, suppose M = 2 and N = 2: \n",
    "\\begin{eqnarray} \\nonumber\n",
    "\t \tE(\\mathbf{w}) &=& \\frac{1}{2} \\left( \\left[w_0, w_1, w_2 \\right] \\left[ \\begin{array}{c c c c} 1 & 1 \\\\ x_1 & x_2  \\\\  x_1^2 & x_2^2  \\end{array}\\right] - \\left[ t_1, t_2 \\right]\\right) \\left( \\left[w_0, w_1, w_2 \\right] \\left[ \\begin{array}{c c c c} 1 & 1 \\\\ x_1 & x_2  \\\\  x_1^2 & x_2^2  \\end{array}\\right] - \\left[ t_1, t_2 \\right]\\right)^T \\nonumber \\\\\n",
    "\t \t&=&  \\frac{1}{2}\\left( \\left[ w_0 + w_1x_1 + w_2x_1^2, w_0 + w_1x_2 + w_2x_2^2 \\right]  - \\left[ t_1, t_2 \\right]\\right) \\nonumber\\\\\n",
    "\t \t& & \\left( \\left[ w_0 + w_1x_1 + w_2x_1^2, w_0 + w_1x_2 + w_2x_2^2 \\right]  - \\left[ t_1, t_2 \\right]\\right)^T \\nonumber\\\\\n",
    "\t \t&=&  \\frac{1}{2}\\left( \\left[ w_0 + w_1x_1 + w_2x_1^2 - t_1, w_0 + w_1x_2 + w_2x_2^2  - t_2\\right] \\right)\\nonumber\\\\\n",
    "\t \t& & \\left( \\left[ w_0 + w_1x_1 + w_2x_1^2 -t_1, w_0 + w_1x_2 + w_2x_2^2 -t_2 \\right] \\right)^T \\nonumber\\\\\n",
    "\t \t&=& \\frac{1}{2}  \\left( \\left(w_0 + w_1x_1 + w_2x_1^2 - t_1\\right)^2 +\\left(w_0 + w_1x_2 + w_2x_2^2 -t_2\\right)^2 \\right) \\nonumber\n",
    "\t \\end{eqnarray}\n",
    "* Then, let us work out the derivative with respect to the vector $\\mathbf{w}$\n",
    "\\begin{eqnarray} \\nonumber\n",
    "\t \t\\frac{\\partial E(\\mathbf{w})}{\\partial \\mathbf{w}} &=& \\left[\\frac{\\partial E(\\mathbf{w})}{\\partial {w}_0}, \\frac{\\partial E(\\mathbf{w})}{\\partial {w}_1}, \\frac{\\partial E(\\mathbf{w})}{\\partial {w}_2} \\right]^T\\nonumber\\\\\n",
    "\t \t%derivative with w_0\n",
    "\t \t&=& \\left[\\frac{1}{2}\\left( 2\\left(w_0 + w_1x_1 + w_2x_1^2 - t_1\\right)  + 2\\left(w_0 + w_1x_2 + w_2x_2^2 -t_2\\right) \\right), \\right. \\nonumber\\\\\n",
    "\t \t%derivative with w_1\n",
    "\t\t& & \\frac{1}{2}\\left( 2\\left(w_0 + w_1x_1 + w_2x_1^2 - t_1\\right)x_1   + 2\\left(w_0 + w_1x_2 + w_2x_2^2 -t_2\\right)x_2 \\right),  \\nonumber\\\\\n",
    "\t \t%derivative with w_2\n",
    "\t\t& & \\frac{1}{2}\\left( 2\\left(w_0 + w_1x_1 + w_2x_1^2 - t_1\\right)x_1^2 + \\left. 2\\left(w_0 + w_1x_2 + w_2x_2^2 -t_2\\right)x_2^2 \\right) \\right]^T \\nonumber\\\\\n",
    "\t\t&=& \\frac{1}{2} 2 \\left[ \\begin{array}{c c c c} 1 & 1 \\\\ x_1 & x_2  \\\\  x_1^2 & x_2^2  \\end{array}\\right]\\left(  \\left[w_0, w_1, w_2 \\right] \\left[ \\begin{array}{c c c c} 1 & 1 \\\\ x_1 & x_2  \\\\  x_1^2 & x_2^2  \\end{array}\\right] - \\left[ t_1, t_2 \\right] \\right)^T\n",
    "\t \\end{eqnarray}\n",
    "\n",
    "\n",
    "\n",
    "Thus, \n",
    "\\begin{eqnarray}\n",
    "\t\t & & 0 =  \\left( \\mathbf{w}^T\\mathbf{X}^T - \\mathbf{t}^T\\right)\\mathbf{X}\\\\\n",
    "        & & 0 =   \\mathbf{w}^T\\mathbf{X}^T\\mathbf{X} - \\mathbf{t}^T\\mathbf{X}\\\\        \n",
    "        & & \\mathbf{t}^T\\mathbf{X} = \\mathbf{w}^T\\mathbf{X}^T\\mathbf{X}\\\\\n",
    "        & & \\mathbf{t}^T\\mathbf{X}\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1} = \\mathbf{w}^T\\mathbf{X}^T\\mathbf{X}\n",
    "        \\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\\\\n",
    "         & &\\mathbf{t}^T\\mathbf{X}\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1} = \\mathbf{w}^T\\\\\n",
    "         & &\\mathbf{w} = \\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{t}\\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "\n",
    "## Apply to data generated from a (noisy) sine curve \n",
    "* Suppose our data actually came from: $t = \\sin(2\\pi x) + \\epsilon$ where $\\epsilon$ is Gaussian zero-mean random noise. \n",
    "* The univariate Gaussian Distribution: \n",
    "\t\\begin{eqnarray}\n",
    "\t\t\\mathcal{N}(x | \\mu, \\sigma^2) = \\frac{1}{(2\\pi \\sigma^2)^{1/2}} \\exp\\left\\{ - \\frac{1}{2\\sigma^2}(x - \\mu)^2\\right\\}\n",
    "\t\\end{eqnarray}\n",
    "\n",
    "<img src=\"figures/fig2.jpg\"  style=\"width: 200px;\"/>\n",
    "\n",
    "* If the noise is zero-mean Gaussian distributed, it is like we are saying there is a Gaussian around the true curve: \n",
    "\n",
    "<img src=\"figures/fig3.jpg\"  style=\"width: 200px;\"/>\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\t\t t = y + \\epsilon\\\\\n",
    "\t\t \\epsilon = t - y\n",
    "\t \\end{eqnarray}\n",
    "\t where\n",
    "\t \\begin{eqnarray}\n",
    "\t \t\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\n",
    " \t \\end{eqnarray}\n",
    " \t thus\n",
    " \t \\begin{eqnarray}\n",
    " \t \t\\mathcal{N}(t-y|0,1) &\\propto& \\exp\\left\\{ -\\frac{1}{2} \\frac{(t-y-0)^2}{1^2} \\right\\}\\\\\n",
    " \t \t&=& \\exp\\left\\{ -\\frac{1}{2} (t-y)^2 \\right\\}\\\\\n",
    " \t \t&=&  \\exp\\left\\{ -E(\\mathbf{w}) \\right\\}\n",
    " \t\\end{eqnarray}\n",
    "\n",
    "* So, the squared error objective function, $E(\\mathbf{w})$, assumes Gaussian noise. \n",
    "* Another way to look at it: $t$ is distributed according to a Gaussian distribution with mean $y$\n",
    "\n",
    "* Also, *What is the multivariate Gaussian distribution?*\n",
    "\n",
    "* First, lets generate data from the *true* underlying function (which, in practice, we would not know)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math \n",
    "\n",
    "def generateUniformData(N, l, u, gVar):\n",
    "\t'''generateUniformData(N, l, u, gVar): Generate N uniformly spaced data points \n",
    "    in the range [l,u) with zero-mean Gaussian random noise with standard deviation gVar'''\n",
    "\t# x = np.random.uniform(l,u,N)\n",
    "\tstep = (u-l)/(N)\n",
    "\tx = np.arange(l+step/2,u+step/2,step)\n",
    "\te = np.random.normal(0,gVar,N)\n",
    "\tt = np.sin(2*math.pi*x) + e\n",
    "\treturn x,t\n",
    "\n",
    "l = 0\n",
    "u = 1\n",
    "N = 1000\n",
    "gVar = .1\n",
    "data_uniform  = np.array(generateUniformData(N, l, u, gVar)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lets plot this data and the underlying *true* function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "%matplotlib inline \n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "def plotData(x1,t1,x2,t2,x3=None,t3=None,legend=[]):\n",
    "\n",
    "    #plot everything\n",
    "    p1 = plt.plot(x1, t1, 'bo') #plot training data\n",
    "    p2 = plt.plot(x2, t2, 'g') #plot true value\n",
    "    if(x3 is not None):\n",
    "        p3 = plt.plot(x3, t3, 'r') #plot training data\n",
    "\n",
    "    #add title, legend and axes labels\n",
    "    plt.ylabel('t') #label x and y axes\n",
    "    plt.xlabel('x')\n",
    "    \n",
    "    if(x3 is None):\n",
    "        plt.legend((p1[0],p2[0]),legend)\n",
    "    else:\n",
    "        plt.legend((p1[0],p2[0],p3[0]),legend)\n",
    "\n",
    "x1 = data_uniform[:,0]\n",
    "t1 = data_uniform[:,1]\n",
    "\n",
    "x2 = np.arange(l,u,0.001)  #get equally spaced points in the xrange\n",
    "t2 = np.sin(2*math.pi*x2) #compute the true function value\n",
    "    \n",
    "plotData(x1, t1, x2, t2,legend=['Training Data', 'True Function'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now lets fit the data using the polynomial curve fitting approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitdata(x,t,M):\n",
    "\t'''fitdata(x,t,M): Fit a polynomial of order M to the data (x,t)'''\t\n",
    "\t#This needs to be filled in\n",
    "\tX = np.array([x**m for m in range(M+1)]).T\n",
    "\tw = np.linalg.inv(X.T@X)@X.T@t\n",
    "\treturn w\n",
    "\n",
    "        \n",
    "M = 10\n",
    "w = fitdata(x1,t1,M)\n",
    "xrange = np.arange(l,u,0.001)  #get equally spaced points in the xrange\n",
    "X = np.array([xrange**m for m in range(w.size)]).T\n",
    "esty = X@w #compute the predicted value\n",
    "plotData(x1,t1,x2,t2,xrange,esty,['Training Data', 'True Function', 'Estimated\\nPolynomial'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Overfitting/Overtraining\n",
    "\n",
    "* In the polynomial curve fitting example, $M$ is the *model order*. \n",
    "* As $M$ increases, there are more parameters (more $w$) to learn and, so, the model becomes more complex.  \n",
    "* As a model is more and more complex, it is more likely to *overfit* or *overtrain*.  This essentially means it may \"memorize\" the input training data (including all of the training data's noise).  \n",
    "* Overfitting means that the performance of the model will likely decrease on unknown test data.  Overfitting means that the \"true\" underlying model of the data is not estimated/learned but instead results in a poor representation that memorizes meaningless noise in the data.\n",
    "* There are two common approaches to avoid overfitting:\n",
    "     1. More data: As you have more and more data, it becomes more and more difficult to \"memorize\" the data and its noise. Often, more data translates to the ability to use a more complex model and avoid overfitting.  However, generally, you need exponentially more data with increases to model complexity.  So, there is a limit to how much this helps.  If you have a very complex model, you need a huge training data set. \n",
    "     2. Regularization: Regularization methods add a penalty term to the error function to discourage overfitting.  These penalty terms encourage small values limiting the ability to overfit.  (This is just a teaser. We will discuss this further in the future.)\n",
    "\n",
    "\n",
    "* You can also *underfit* your data.  When you underfit, your model complexity is not complex enough to model all of the complexities in your data. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Beer Foam Example\n",
    "\n",
    "* Lets go through the Polynomial Curve fitting again with another example\n",
    "* Obtained from: http://www.stat.ufl.edu/~winner/datasets.html \n",
    "\n",
    "Source: A. Leike (2002). \"Demonstration of the Exponential Decay Law Using Beer Froth,\" European Journal of Physics, Vol. 23, #1, pp. 21-26\n",
    "\n",
    "Description: Measurements of wet foam height and beer height at various time points for 3 brands of beer. Author fits exponential decay model: $H(t) = H(0)e^{-\\lambda t}$\n",
    "\n",
    "Variables/Columns:\n",
    "<li> Time from pour (seconds)  4-8\n",
    "<li> Erdinger Weissbier foam height (cm)  10-16\n",
    "<li> Augustinerbrau Munchen foam height (cm)    18-24\n",
    "<li> Budweiser foam height (cm)    26-32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "beerData = np.loadtxt('beer_foam.dat.txt')\n",
    "\n",
    "plt.scatter(beerData[:,0], beerData[:,1], color = \"red\")\n",
    "plt.scatter(beerData[:,0], beerData[:,2], color = \"blue\")\n",
    "plt.scatter(beerData[:,0], beerData[:,3], color = \"orange\")\n",
    "\n",
    "#Then we can fit the data using the polynomial curve fitting method we derived\n",
    "x = beerData[:,0]\n",
    "t = beerData[:,2]\n",
    "w = fitdata(x,t,M=4)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let us use the weights in test\n",
    "xrange = np.arange(beerData[0,0],beerData[beerData.shape[0]-1,0],0.001)  #get equally spaced points in the xrange\n",
    "X = np.array([xrange**m for m in range(w.size)]).T\n",
    "esty = X@w #compute the predicted value\n",
    "\n",
    "plotData(x,t,xrange,esty,legend=['Training Data','Estimated\\nPolynomial'])\n",
    "\n",
    "#What will the foam height be at t = ____? \n",
    "t = 500\n",
    "x_test = np.array([t**m for m in range(w.size)]).T\n",
    "print(x_test)\n",
    "predicted_height = x_test@w\n",
    "print(predicted_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
