{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood and Maximum A Posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We looked at the regularization term as a *penalty* term in the objective function.  There is another way to interpret the regularization term as well.  Specifically, there is a *Bayesian* interpretation. \n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\min E^{\\ast}(\\mathbf{w}) &=& \\max -E^{\\ast}(\\mathbf{w})\\\\\n",
    "& =& \\max \\exp \\left\\{ -E^{\\ast}(\\mathbf{w})\\right\\}\\\\\n",
    "&=& \\max \\exp \\left\\{ -\\frac{1}{2}\\sum_{n=1}^N \\left( y(x_n, \\mathbf{w}) - t_n \\right)^2 - \\frac{\\lambda}{2}\\left\\| \\mathbf{w} \\right\\|^2_2 \\right\\}\\\\\n",
    "&=& \\max \\exp \\left\\{ -\\frac{1}{2}\\sum_{n=1}^N \\left( y(x_n, \\mathbf{w}) - t_n \\right)^2 \\right\\}\\exp\\left\\{-\\frac{1}{2}\\lambda\\left\\| \\mathbf{w} \\right\\|^2_2\\right\\}\\\\\n",
    "&=& \\max \\prod_{n=1}^N \\exp \\left\\{ -\\frac{1}{2} \\left( y(x_n, \\mathbf{w}) - t_n \\right)^2 \\right\\}\\exp\\left\\{-\\frac{1}{2}\\lambda\\left\\| \\mathbf{w} \\right\\|^2_2\\right\\}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So, this is a maximization of the *data likelihood* with a *prior*: $p(\\mathbf{X}|\\mathbf{w})p(\\mathbf{w})$\n",
    "\n",
    "* *Method of Maximum Likelihood:*\n",
    "    * A *data likelihood* is how likely the data is given the parameter set\n",
    "    * So, if we want to maximize how likely the data is to have come from the model we fit, we should find the parameters that maximize the likelihood\n",
    "    * A common trick to maximizing the likelihood is to maximize the log likelihood.  Often makes the math much easier.  *Why can we maximize the log likelihood instead of the likelihood and still get the same answer?*\n",
    "    * Consider: $\\max \\ln \\exp \\left\\{ -\\frac{1}{2}\\left(y(x_n, \\mathbf{w}) - t_n\\right)^2\\right\\}$ We go back to our original objective. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Method of Maximum A Posteriori (MAP):*\n",
    "    * Bayes Rule: $p(Y|X) = \\frac{p(X|Y)p(Y)}{p(X)}$\n",
    "    * Consider: $p(\\mathbf{w}|\\mathscr{D}) = \\frac{p(\\mathscr{D}|\\mathbf{w})p(\\mathbf{w})}{p(\\mathscr{D})}$, i.e., posterior $\\propto$ likelihood $\\times$ prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood vs. Maximum A Posteriori (MAP)\n",
    "\n",
    "* Lets look at this in terms of binary variables, e.g., Flipping a coin:  $X =1$ is heads, $X=0$ is tails\n",
    "* Let $\\mu$ be the probability of heads.  If we know $\\mu$, then: $P(x = 1 |\\mu) = \\mu$ and $P(x = 0|\\mu) = 1-\\mu$\n",
    "\\begin{eqnarray}\n",
    "P(x|\\mu) = \\mu^x(1-\\mu)^{1-x} = \\left\\{\\begin{array}{c c}\\mu & \\text{ if } x=1 \\\\ 1-\\mu & \\text{ if } x = 0 \\end{array}\\right.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is called the *Bernoulli* distribution.  The mean and variance of a Bernoulli distribution is: \n",
    "\\begin{equation}\n",
    "E[x] = \\mu\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "E\\left[(x-\\mu)^2\\right] = \\mu(1-\\mu)\n",
    "\\end{equation}\n",
    "* So, suppose we conducted many Bernoulli trials (e.g., coin flips) and we want to estimate $\\mu$\n",
    "\n",
    "### Method: Maximum Likelihood\n",
    "\\begin{eqnarray}\n",
    "p(\\mathscr{D}|\\mu) &=& \\prod_{n=1}^N p(x_n|\\mu) \\\\\n",
    "&=& \\prod_{n=1}^N \\mu^{x_n}(1-\\mu)^{1-x_n}\n",
    "\\end{eqnarray}\n",
    "\n",
    "* Maximize : (*What trick should we use?*)\n",
    "\\begin{eqnarray}\n",
    "\\mathscr{L} = \\sum_{n=1}^N x_n \\ln \\mu + (1-x_n)\\ln(1-\\mu)\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial \\mu} =  0 &=& \\frac{1}{\\mu}\\sum_{n=1}^N x_n - \\frac{1}{1-\\mu }\\sum_{n=1}^N (1 - x_n)\\\\\n",
    "0 &=& \\frac{(1-\\mu) \\sum_{n=1}^N x_n - \\mu \\sum_{n=1}^N (1- x_n)}{\\mu(1-\\mu)}\\\\\n",
    "0 &=& \\sum_{n=1}^N x_n - \\mu \\sum_{n=1}^N x_n - \\mu \\sum_{n=1}^N 1 + \\mu \\sum_{n=1}^N x_n\\\\\n",
    "0 &=& \\sum_{n=1}^N x_n - \\mu N\\\\\n",
    "\\mu &=& \\frac{1}{N}\\sum_{n=1}^N x_n = \\frac{m}{N}\n",
    "\\end{eqnarray}\n",
    "where $m$ is the number of successful trials. \n",
    "\n",
    "* So, if we flip a coin 1 time and get heads, then $\\mu = 1$ and probability of getting tails is 0.  *Would you believe that? We need a prior!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method: Maximum A Posteriori: \n",
    "\n",
    "* Look at several independent trials.  Consider N = 3 and m = 2 (N is number of trials, m is number of successes) and look at all ways to get 2 H and 1 T: \n",
    "     * H H T  $\\rightarrow \\mu \\mu (1-\\mu) = \\mu^2(1-\\mu)$\n",
    "     * H T H  $\\rightarrow \\mu  (1-\\mu) \\mu  = \\mu^2(1-\\mu)$\n",
    "     * T H H $\\rightarrow (1-\\mu) \\mu   \\mu  = \\mu^2(1-\\mu)$\n",
    "\n",
    "* $\\left(\\begin{array}{c} 3 \\\\ 2 \\end{array}\\right) \\mu^2(1-\\mu) \\rightarrow \\left(\\begin{array}{c} N \\\\ m \\end{array}\\right) \\mu^m(1-\\mu)^{N-m} = \\frac{N!}{(N-m)!m!}\\mu^m(1-\\mu)^{N-m} $\n",
    "* This is the Binomial Distribution, gives the probability of $m$ observations of $x=1$ out of N independent trails\n",
    "* So, what we saw is that we need a prior.  We want to incorporate our prior belief. Let us place a prior on $\\mu$\n",
    "\\begin{equation}\n",
    "Beta(\\mu|a,b) = \\frac{\\Gamma(a + b)}{\\Gamma(a)\\Gamma(b)}\\mu^{a-1}(1-\\mu)^{b-1}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "E[\\mu] = \\frac{a}{a + b}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "Var[\\mu] = \\frac{ab}{(a+b)^2(a+b+1)}\n",
    "\\end{equation}\n",
    "* Note: $\\Gamma(x) = \\int_0^\\infty u^{x-1}e^{-u} du$ and when $x$ is an integer, then it simplifys to $x!$\n",
    "* Calculation of the posterior, Take $N = m + l$ observations:\n",
    "\\begin{eqnarray}\n",
    "p(\\mu | m, l, a, b) &\\propto& Bin(m,l|\\mu)Beta(\\mu|a,b) \\\\\n",
    "&\\propto& \\mu^m(1-\\mu)^l\\mu^{a-1}(1-\\mu)^{b-1}\\\\\n",
    "&=& \\mu^{m+a-1}(1-\\mu)^{l+b-1}\n",
    "\\end{eqnarray}\n",
    "* What does this look like?  Beta: $a \\leftarrow m+a$, $b \\leftarrow l+b$\n",
    "* So, what's the posterior? \n",
    "\\begin{equation}\n",
    "p(\\mu | m, l, a, b) = \\frac{\\Gamma(m+a+l+b)}{\\Gamma(m+a)\\Gamma(l+b)}\\mu^{m+a-1}(1-\\mu)^{l+b-1}\n",
    "\\end{equation}\n",
    "* *Conjugate Prior Relationship:* When the posterior is the same form as the prior\n",
    "* Now we can maximize the (log of the) posterior: \n",
    "\\begin{eqnarray}\n",
    "\\max_\\mu ((m+a-1) \\ln \\mu + (l+b-1) \\ln (1-\\mu))\n",
    "\\end{eqnarray}\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial \\mu} =  0&=& \\frac{m + a -1}{\\mu} - \\frac{l + b - 1}{1-\\mu}\\\\\n",
    "&=& (1-\\mu)(m+a-1) - \\mu(l+b-1)\\\\\n",
    "&=& (m+a-1) - \\mu(m+a-1) - \\mu(l+b-1)\\\\\n",
    "\\mu &=& \\frac{m+a-1}{m+a+l+b-2}\n",
    "\\end{eqnarray}\n",
    "* This is the MAP solution.  *So, what happens now when you flip one heads, two heads, etc.?*\n",
    "* Discuss online updating of the prior.  Eventually the data takes over the prior. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "Frequentist/Maximum Likelihood Probability of Heads:1.0\n",
      "Bayesian/MAP Probability of Heads:0.6666666666666666\n",
      "Hit enter to continue...\n",
      "\n",
      "[1, 0]\n",
      "Frequentist/Maximum Likelihood Probability of Heads:0.5\n",
      "Bayesian/MAP Probability of Heads:0.5\n",
      "Hit enter to continue...\n",
      "\n",
      "[1, 0, 0]\n",
      "Frequentist/Maximum Likelihood Probability of Heads:0.3333333333333333\n",
      "Bayesian/MAP Probability of Heads:0.4\n",
      "Hit enter to continue...\n",
      "\n",
      "[1, 0, 0, 0]\n",
      "Frequentist/Maximum Likelihood Probability of Heads:0.25\n",
      "Bayesian/MAP Probability of Heads:0.3333333333333333\n",
      "Hit enter to continue...\n",
      "\n",
      "[1, 0, 0, 0, 1]\n",
      "Frequentist/Maximum Likelihood Probability of Heads:0.4\n",
      "Bayesian/MAP Probability of Heads:0.42857142857142855\n",
      "Hit enter to continue...\n",
      "\n",
      "[1, 0, 0, 0, 1, 0]\n",
      "Frequentist/Maximum Likelihood Probability of Heads:0.3333333333333333\n",
      "Bayesian/MAP Probability of Heads:0.375\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math \n",
    "%matplotlib inline   \n",
    "\n",
    "def plotBeta(a,b):\n",
    "\t'''plotBeta(a=1,b=1): Plot plot beta distribution with parameters a and b'''\n",
    "\txrange = np.arange(0,1,0.001)  #get equally spaced points in the xrange\n",
    "\tnormconst = math.gamma(a+b)/(math.gamma(a)*math.gamma(b))\n",
    "\tbeta = normconst*xrange**(a-1)*(1-xrange)**(b-1)\n",
    "\tfig = plt.figure()\n",
    "\tp1 = plt.plot(xrange,beta, 'g')\n",
    "\tplt.show()\n",
    "\n",
    "\n",
    "#Beta Distribution\n",
    "plotBeta(2,4);\n",
    "\n",
    "trueMu = 0.5\n",
    "numFlips = 10\n",
    "priorA = 2\n",
    "priorB = 2\n",
    "flipResult = []\n",
    "for flip in range(numFlips):\n",
    "    flipResult.append(np.random.binomial(1,trueMu,1)[0])\n",
    "    print(flipResult)\n",
    "    print('Frequentist/Maximum Likelihood Probability of Heads:' + str(sum(flipResult)/len(flipResult)))\n",
    "    print('Bayesian/MAP Probability of Heads:' + str((sum(flipResult)+priorA-1)/(len(flipResult)+priorA+priorB-2)))\n",
    "    input(\"Hit enter to continue...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gaussian Distribution:\n",
    "* Consider a univariate Gaussian distribution:\n",
    "\\begin{equation}\n",
    "\\mathscr{N}(x|\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left\\{ -\\frac{1}{2}\\frac{(x-\\mu)^2}{\\sigma^2} \\right\\}\n",
    "\\end{equation}\n",
    "* $\\sigma^2$ is the variance OR $\\frac{1}{\\sigma^2}$ is the *precision*\n",
    "* So, as $\\lambda$ gets big, variance gets smaller/tighter.  As $\\lambda$ gets small, variance gets larger/wider.\n",
    "* The Gaussian distribution is also called the *Normal* distribution. \n",
    "* We will often write $N(x|\\mu, \\sigma^2)$ to refer to a Gaussian with mean $\\mu$ and variance $\\sigma^2$.\n",
    "* *What is the multi-variate Gaussian distribution?* \n",
    "\n",
    "* What is the expected value of $x$ for the Gaussian distribution?\n",
    "\\begin{eqnarray}\n",
    "E[x] &=& \\int x p(x) dx \\\\\n",
    "     &=& \\int x \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left\\{ -\\frac{1}{2}\\frac{(x-\\mu)^2}{\\sigma^2} \\right\\} dx\n",
    "\\end{eqnarray}\n",
    "* *Change of variables:*  Let\n",
    "\\begin{eqnarray}\n",
    "y &=& \\frac{x-\\mu}{\\sigma} \\rightarrow x = \\sigma y + \\mu\\\\\n",
    "dy &=& \\frac{1}{\\sigma} dx \\rightarrow dx = \\sigma dy\n",
    "\\end{eqnarray}\n",
    "* Plugging this into the expectation: \n",
    "\\begin{eqnarray}\n",
    "E[x] &=& \\int \\left(\\sigma y + \\mu  \\right)\\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left\\{ - \\frac{1}{2} y^2 \\right\\} \\sigma dy \\\\\n",
    "&=& \\int \\frac{\\sigma y}{\\sqrt{2\\pi}} \\exp\\left\\{ - \\frac{1}{2} y^2 \\right\\} dy + \\int \\frac{\\mu}{\\sqrt{2\\pi}} \\exp\\left\\{ - \\frac{1}{2} y^2 \\right\\} dy \n",
    "\\end{eqnarray}\n",
    "* The first term is an odd function: $f(-y) = -f(y)$  So, $E[x] = 0 + \\mu = \\mu$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## MLE of Mean of Gaussian\n",
    "\n",
    "*  Let $\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_N$ be samples from a multi-variance Normal distribution with known covariance matrix and an unknown mean.  Given this data, obtain the ML estimate of the mean vector. \n",
    "\t\\begin{equation}\n",
    "\tp(\\mathbf{x}_k| {{\\mu}}) = \\frac{1}{(2\\pi)^{\\frac{l}{2}}\\left| \\Sigma \\right|^{\\frac{1}{2}}}\\exp\\left( -\\frac{1}{2}(\\mathbf{x}_k - {{\\mu}})^T\\Sigma^{-1}(\\mathbf{x}_k - {\\mu})\\right)\n",
    "\t\\end{equation}\n",
    "* We can define our likelihood given the $N$ data points.  We are assuming these data points are drawn independently but from an identical distribution (i.i.d.):\n",
    "\t\\begin{equation}\n",
    "\t\\prod_{n=1}^N p(\\mathbf{x}_n| {{\\mu}}) = \\prod_{n=1}^N \\frac{1}{(2\\pi)^{\\frac{l}{2}}\\left| \\Sigma \\right|^{\\frac{1}{2}}}\\exp\\left( -\\frac{1}{2}(\\mathbf{x}_n - {{\\mu}})^T\\Sigma^{-1}(\\mathbf{x}_n - {\\mu})\\right)\n",
    "\t\\end{equation}\n",
    "*  We can apply our \"trick\" to simplify\n",
    "\t\\begin{eqnarray}\n",
    "\t\\mathscr{L} &=& \\ln \\prod_{n=1}^N p(\\mathbf{x}_n| {{\\mu}}) = \\ln \\prod_{n=1}^N \\frac{1}{(2\\pi)^{\\frac{l}{2}}\\left| \\Sigma \\right|^{\\frac{1}{2}}}\\exp\\left( -\\frac{1}{2}(\\mathbf{x}_n - {{\\mu}})^T\\Sigma^{-1}(\\mathbf{x}_n - {\\mu})\\right)\\\\\n",
    "\t&=& \\sum_{n=1}^N  \\ln \\frac{1}{(2\\pi)^{\\frac{l}{2}}\\left| \\Sigma \\right|^{\\frac{1}{2}}}\\exp\\left( -\\frac{1}{2}(\\mathbf{x}_n - {{\\mu}})^T\\Sigma^{-1}(\\mathbf{x}_n - {\\mu})\\right)\\\\\n",
    "\t&=& \\sum_{n=1}^N  \\left( \\ln \\frac{1}{(2\\pi)^{\\frac{l}{2}}\\left| \\Sigma \\right|^{\\frac{1}{2}}} + \\left( -\\frac{1}{2}(\\mathbf{x}_n - {{\\mu}})^T\\Sigma^{-1}(\\mathbf{x}_n - {\\mu})\\right) \\right) \\\\\n",
    "\t&=&  - N \\ln (2\\pi)^{\\frac{l}{2}}\\left| \\Sigma \\right|^{\\frac{1}{2}} + \\sum_{n=1}^N  \\left( -\\frac{1}{2}(\\mathbf{x}_n - {{\\mu}})^T\\Sigma^{-1}(\\mathbf{x}_n - {\\mu}) \\right) \n",
    "\t\\end{eqnarray}\n",
    "* Now, lets maximize:\n",
    "\t\\begin{eqnarray}\n",
    "\t\\frac{\\partial \\mathscr{L}}{\\partial \\mu} &=& \\frac{\\partial}{\\partial \\mu} \\left[- N \\ln (2\\pi)^{\\frac{l}{2}}\\left| \\Sigma \\right|^{\\frac{1}{2}} + \\sum_{n=1}^N  \\left( -\\frac{1}{2}(\\mathbf{x}_n - {{\\mu}})^T\\Sigma^{-1}(\\mathbf{x}_n - {\\mu}) \\right)\\right] = 0 \\\\\n",
    "\t&\\rightarrow& \\sum_{n=1}^N  \\Sigma^{-1}(\\mathbf{x}_n - {\\mu}) = 0\\\\\n",
    "\t&\\rightarrow& \\sum_{n=1}^N  \\Sigma^{-1}\\mathbf{x}_n  = \\sum_{n=1}^N  \\Sigma^{-1} {\\mu}\\\\\n",
    "\t&\\rightarrow& \\Sigma^{-1} \\sum_{n=1}^N \\mathbf{x}_n  = \\Sigma^{-1} {\\mu} N\\\\\n",
    "\t&\\rightarrow& \\sum_{n=1}^N \\mathbf{x}_n  = {\\mu} N\\\\\n",
    "\t&\\rightarrow& \\frac{\\sum_{n=1}^N \\mathbf{x}_n}{N} = {\\mu}\\\\\n",
    "\t\\end{eqnarray}\n",
    "* So, the ML estimate of $\\mu$ is the sample mean!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP of Mean of Gaussian\n",
    "\n",
    "* To get a MAP estimate of the mean of a Gaussian, we apply a prior distribution and maximize the posterior. \n",
    "* Lets use a Gaussian prior on the mean (because it has a *conjugate prior* relationship)\n",
    "\n",
    "\\begin{eqnarray}\n",
    "p(\\mu|X, \\mu_0, \\sigma_0^2, \\sigma^2) &\\propto& \\mathscr{N}(X|\\mu, \\sigma^2)\\mathscr{N}(\\mu|\\mu_0, \\sigma_0^2)\\\\\n",
    "&=& \\prod_{n=1}^N \\left(\\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left\\{-\\frac{1}{2\\sigma^2}\\left(x_n-\\mu\\right)^2 \\right\\}\\right)\\frac{1}{\\sqrt{2\\pi \\sigma_0^2}} \\exp\\left\\{-\\frac{1}{2\\sigma_0^2}\\left(\\mu-\\mu_0\\right)^2 \\right\\}\\nonumber\\\\\n",
    "\\mathscr{L} &=& -\\frac{N}{2}\\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{n=1}^N(x_n-\\mu)^2 - \\frac{N}{2}\\ln(2\\pi \\sigma_0^2) - \\frac{N}{2\\sigma_0^2}(\\mu - \\mu_0)^2\\\\\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial \\mu} &=&  - \\frac{N}{\\sigma^2}\\mu - \\frac{N}{\\sigma_0^2}\\mu + \\frac{N}{\\sigma_0^2}\\mu_0 + \\frac{1}{\\sigma^2}\\sum_{n=1}^N x_n = 0\\\\\n",
    "N\\mu\\left(\\frac{N\\sigma_0^2 + \\sigma^2}{\\sigma^2\\sigma_0^2} \\right) &=& \\frac{1}{\\sigma^2}\\sum_{n=1}^N x_n + \\frac{1}{\\sigma_0^2}\\mu_0 \\\\\n",
    "\\mu_{MAP} &=& \\frac{\\sigma_0^2}{N\\sigma_0^2 + N\\sigma^2}\\sum_{n=1}^N x_n + \\frac{\\mu_0\\sigma^2}{N\\sigma_0^2 + N\\sigma^2}\n",
    "\\end{eqnarray}\n",
    "\n",
    "* *Does this result make sense?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
